# RAG LLM по заметкам

Как это работает:
1. В папке `notes/` находятся заметки в формате Markdown/TXT.
2. Заметки индексируются в Chroma векторное хранилище.
3. При запросе к LLM, запрос обрабатывается и отправляется в OpenRouter.
4. OpenRouter обрабатывает запрос и возвращает ответ.
5. Ответ обрабатывается и отправляется в Telegram Bot.

Все написано на Python.

## Как запускать

1. Установить зависимости:
```bash
pip install -e .
```
2. Создать `.env` из шаблона:
```bash
cp env.example .env
```
3. Запустить RAG API:
```bash
python scripts/run_rag.py
```
4. Запустить Telegram Bot:
```bash
python scripts/run_bot.py
```

## Как использовать

1. Отправить сообщение в Telegram бот.
2. Бот будет отвечать на запрос используя контекст из заметок.

> **Важно**: Бот был настроен так, что он будет отвечать ТОЛЬКО на основе контекста из заметок. Если в заметках нет информации по запросу, бот так и скажет, что он ничего не нашел. Т. е. никакого придумывания ответов.
> 
> По факту, бот пытается воссоздать текст из заметок 1:1 для ответа на запрос.

## Модель

В качестве модели используется OpenRouter. На момент 21.01.2026 это проверялось на `xiaomi/mimo-v2-flash:free` модели
https://openrouter.ai/xiaomi/mimo-v2-flash:free/api

Дополнительно для работы модели используется embedding модель `paraphrase-multilingual-MiniLM-L12-v2`.

Можно также поиграться с `top_k` параметром — это количество наиболее релевантных чанков, извлекаются из векторной базы при поиске ответа (от 1 до 20)

По умолчанию `top_k = 4` (настраивается через `TOP_K_DEFAULT` в `.env`).

Слишком маленький `top_k` (например, 1-2) — может не хватить контекста.

Слишком большой `top_k` (например, 20) — в контекст попадут менее релевантные чанки, может быть шум и превышение лимита токенов LLM.

## Примеры

В этом репозитории уже есть нагенеренные заметки в папке `notes/`. Ниже пару примеров работы бота на основе них:


